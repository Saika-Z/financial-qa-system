from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document
import os
import glob
import core.database as db


class DataPipelineService:
    def __init__(self):
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (Embedding Model)
        print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {db.EMBEDDING_MODEL_NAME}...")
        # HuggingFaceEmbeddings åŒ…è£…äº† sentence-transformers åº“
        self.embeddings = HuggingFaceEmbeddings(model_name=db.EMBEDDING_MODEL_NAME)
        
        # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥ (ä½¿ç”¨ ChromaDB ä½œä¸ºæœ¬åœ°æ•°æ®åº“)
        # Chroma.from_documents æ˜¯å¯¼å…¥æ•°æ®çš„æ ‡å‡†æ–¹å¼
        self.vector_store = Chroma(
            persist_directory=db.VECTOR_DB_DIR, 
            embedding_function=self.embeddings
        )
        print(f"å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„: {db.VECTOR_DB_DIR}")

    def load_and_chunk_data(self, config_key):
        """
        åŠ è½½æŒ‡å®šæ•°æ®æºçš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶æ ¹æ®é…ç½®è¿›è¡Œåˆ†å—ã€‚
        """
        config = db.SOURCES_CONFIG[config_key]
        all_documents = []
        
        # 1. åˆå§‹åŒ–é€’å½’æ–‡æœ¬åˆ†å‰²å™¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config['chunk_size'],
            chunk_overlap=config['chunk_overlap'],
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
        # 2. éå†ç›®å½•ä¸‹æ‰€æœ‰ TXT æ–‡ä»¶
        for file_path in glob.glob(os.path.join(config['path'], '*.txt')):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                source_name = os.path.basename(file_path)
                
                # 3. æ‰§è¡Œåˆ†å—
                chunks = text_splitter.split_text(text)
                
                # 4. è½¬æ¢ä¸º LangChain Document æ ¼å¼å¹¶æ·»åŠ å…ƒæ•°æ®
                for i, chunk in enumerate(chunks):
                    # ç¡®ä¿æ¯ä¸ªå—éƒ½é™„å¸¦å…³é”®å…ƒæ•°æ®
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "source": source_name,
                            "type": config['doc_type'],
                            "chunk_id": f"{source_name}_{i+1}",
                            "chunk_size": len(chunk),
                            # æ‚¨å¯ä»¥è§£ææ–‡ä»¶å†…å®¹å¤´éƒ¨çš„ TITLE, URL ç­‰ä¿¡æ¯ï¼Œç„¶ååœ¨æ­¤å¤„æ·»åŠ 
                            # ç¤ºä¾‹ï¼š'title': "P/E Ratio Definition" 
                        }
                    )
                    all_documents.append(doc)
                
                print(f"   -> {source_name} åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—ã€‚")

            except Exception as e:
                print(f"   âŒ å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        return all_documents

    def ingest_data_into_vector_store(self):
        """
        ä¸»å‡½æ•°ï¼šç»Ÿä¸€å¤„ç†æ‰€æœ‰æ•°æ®æºå¹¶å¯¼å…¥å‘é‡æ•°æ®åº“ã€‚
        """
        print("--- å¯åŠ¨ RAG çŸ¥è¯†åº“å¯¼å…¥æµç¨‹ ---")
        all_chunks = []

        for key in db.SOURCES_CONFIG.keys():
            print(f"å¤„ç†æ•°æ®æº: {key.upper()}...")
            chunks = self.load_and_chunk_data(key)
            all_chunks.extend(chunks)

        if not all_chunks:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„æ–‡æœ¬å—ã€‚è¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶ã€‚")
            return

        # 5. åµŒå…¥ (Embedding) å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        print(f"\nå¼€å§‹åµŒå…¥å’Œå­˜å‚¨ {len(all_chunks)} ä¸ªæ–‡æœ¬å—...")
        # Chroma.add_documents ä¼šè‡ªåŠ¨è°ƒç”¨ self.embeddings å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡
        self.vector_store.add_documents(all_chunks)
        
        # 6. æŒä¹…åŒ–å­˜å‚¨
        self.vector_store.persist()
        print(f"ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆã€‚æ€»è®¡ {len(all_chunks)} ä¸ªå—å·²å­˜å‚¨ã€‚")
        
        # 

# --- è„šæœ¬æ‰§è¡Œç¤ºä¾‹ ---
if __name__ == "__main__":
    # æ³¨æ„ï¼šåœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæ‚¨åº”è¯¥ç¡®ä¿æ‰€æœ‰æ¸…æ´—åçš„æ–‡ä»¶å·²å­˜åœ¨äº DATA_BASE_DIR ä¸‹
    # æ¨¡æ‹Ÿåˆ›å»ºç›®å½•ä»¥ä¾¿ä»£ç è¿è¡Œ
    for key in db.SOURCES_CONFIG.keys():
        os.makedirs(db.SOURCES_CONFIG[key]['path'], exist_ok=True)
    
    # ç¡®ä¿ data/kb å­˜åœ¨
    os.makedirs(os.path.join(os.getcwd(), 'data', 'kb'), exist_ok=True)
    
    # å‡è®¾ï¼šæ‚¨å·²ç»å°†æ¸…æ´—åçš„ TXT æ–‡ä»¶æ”¾å…¥ç›¸åº”çš„ç›®å½•ä¸­
    
    pipeline = DataPipelineService()
    pipeline.ingest_data_into_vector_store()